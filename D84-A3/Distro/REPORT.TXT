CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

15 AIUs (Artificial Intelligence Units)
toward the 35% assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Bisesar, Shevlin

Student Name 2 (last, first): Korapaty, Venkat

Student number 1: 1001307766

Student number 2: 1001365060

UTORid 1: bisesars

UTORid 2: korapaty

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: Shevlin Bisesar	Venkat Korapaty


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
	We have a simple reward function, gives positive number for getting cheese,
	negative number for dying to a cat, and 0 otherwise.

2 .- These are multiple experiments (once you are sure your 
     QLearning code is working!)

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 10000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 0.109070

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate: 0.419864

     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 0.229623

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate: 0.836088

     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?
	The mouse would improve, but it would not become invisible. Similar to
	gradient descent, the rate of success would increase, but the amount of
	time it would take makes it insignificant and not worth while. There are
	also certain states where it's impossible to win, such as the cat has you
	cornered somewhere.

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.
	
     Mouse Winning Rate:-nan

     (2 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.
	
     Mouse Winning Rate:-nan

     Average rate for Experiement 3 and Experiment 4: (-nan + -nan)/2 = -nan

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
	We trained assuming our static environment was 1522, but the seed changed,
	so our QTable failed on the new seed.

5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts): 0.047886

     Mouse Winning Rate (from evaluation after training): 0.114471

     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
	There are far more states in experiment 5 that we need to reach, which is much
	harder. In experiment to, it was an 8x8, that are far fewer states. We would require
	a lot more training time. An 8x8 has 262144 states, where as a 16x16 has
	16777216 states.

6 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above
	No, Q-Learning is limited to static enviroments.  Looking at the above experiments we can see 		on one environment will yield bad results when evaluated on another, which would mean to do 
	we would have to store training on multiple environments, which would no longer be training 
	for a changing environment.

7 .- (3 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win.
	One feature is estimated distance to cat and the other is we also use A* for distance
	to the cheese. These features are useful because tells the mouse that a cat approximately
	being close is bad and closing the distance from the cheese is good. For bigger map sizes,
	16x16 and 32x32, we randomly run A*, as it becomes computationally expensive for more bigger
	maps since we need to run more iterations to learn properly.

8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts): 0.617273
     
     Mouse Winning Rate (from evaluation after training): 0.668505

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 0.703674

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 0.749415

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes 
     to the environment?
	Feature-based Q-Learning is strong in its ability to deal with a dynamic
	environment. The success rates we got were higher than the original run,
	showing that we were able to learn useful features well to help predict
	what the mouse should do.
     
9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts): 0.685552
     
     Mouse Winning Rate (from evaluation after training): 0.715245
  
     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training): 0.736210

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training): 0.667415

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs. 
     feature-based Q-learning?
	Feature-based Q-learning is far more versatile and robust due to the fact that
	it can handle changing environments. Standard Q-Learning is good given that your
	state space isn't too large, making it computationally infeasible, and you expect
	the environment to not change. Feature-based Q-Learning doesn't suffer from
	high possible state spaces and still preserves most of its robustness when state space
	increases. A tradeoff for using feature-based Q-learning is that if you don't have
	meaningful and informative features, it may fall short.

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?
	Simulation. We simulate the environment we want our robot to be in, and run our
	feature-based Q-learning with the simulation, and use those weights for a real
	robot.
      
      
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	Partial		Not done

QLearn 				x
 update

Reward				x
 function

Decide				x
 action

featureEval			x

evaluateQsa			x

maxQsa_prime			x

Qlearn_features			x

decideAction_features		x

_____________________________________________________

Marking:

(10 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(20 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(25 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 100


